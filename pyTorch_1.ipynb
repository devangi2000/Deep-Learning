{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNV8gsLgaEYAR1tndpYmEBF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devangi2000/Deep-Learning/blob/master/pyTorch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNLE10fcxsY"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as functional\n",
        "\n",
        "#generate fake data and add noise\n",
        "x = torch.randn(200, 1)\n",
        "y = x.pow(4) + 0.05*torch.randn(x.size())\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyMXID1vdK_Z"
      },
      "source": [
        "epoch = 100\n",
        "learning_rate = 0.05\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_input, n_hidden, n_output):\n",
        "    super(Net, self).__init__()\n",
        "    self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
        "    self.output = torch.nn.Linear(n_hidden, n_output)\n",
        "\n",
        "  def forward(self, input):\n",
        "    hidden = functional.relu(self.hidden(input))\n",
        "    output = self.output(hidden)\n",
        "    return output\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9agpdj9fB79",
        "outputId": "5a7aefd4-50a5-4090-f6f0-f95b17dec172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "net = Net(1,15,1)\n",
        "print(net)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (hidden): Linear(in_features=1, out_features=15, bias=True)\n",
            "  (output): Linear(in_features=15, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCbULxQkfkA6"
      },
      "source": [
        "lossFunc = torch.nn.MSELoss()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMICNvjUf9qD",
        "outputId": "357437b2-2a8e-4d62-8ebb-154071381507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(epoch):\n",
        "  output = net(x)\n",
        "  loss = lossFunc(output, y)\n",
        "  net.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  for f in net.parameters():\n",
        "    f.data.sub_(learning_rate * f.grad.data)\n",
        "\n",
        "  print(i, 'th loss is ', loss.squeeze())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 th loss is  tensor(98.3496, grad_fn=<SqueezeBackward0>)\n",
            "1 th loss is  tensor(82.6012, grad_fn=<SqueezeBackward0>)\n",
            "2 th loss is  tensor(74.3282, grad_fn=<SqueezeBackward0>)\n",
            "3 th loss is  tensor(68.8858, grad_fn=<SqueezeBackward0>)\n",
            "4 th loss is  tensor(64.7003, grad_fn=<SqueezeBackward0>)\n",
            "5 th loss is  tensor(60.9566, grad_fn=<SqueezeBackward0>)\n",
            "6 th loss is  tensor(57.4967, grad_fn=<SqueezeBackward0>)\n",
            "7 th loss is  tensor(54.2187, grad_fn=<SqueezeBackward0>)\n",
            "8 th loss is  tensor(51.0546, grad_fn=<SqueezeBackward0>)\n",
            "9 th loss is  tensor(47.9745, grad_fn=<SqueezeBackward0>)\n",
            "10 th loss is  tensor(44.9959, grad_fn=<SqueezeBackward0>)\n",
            "11 th loss is  tensor(42.1275, grad_fn=<SqueezeBackward0>)\n",
            "12 th loss is  tensor(39.4172, grad_fn=<SqueezeBackward0>)\n",
            "13 th loss is  tensor(36.8954, grad_fn=<SqueezeBackward0>)\n",
            "14 th loss is  tensor(34.5634, grad_fn=<SqueezeBackward0>)\n",
            "15 th loss is  tensor(32.4121, grad_fn=<SqueezeBackward0>)\n",
            "16 th loss is  tensor(30.4177, grad_fn=<SqueezeBackward0>)\n",
            "17 th loss is  tensor(28.5851, grad_fn=<SqueezeBackward0>)\n",
            "18 th loss is  tensor(26.9041, grad_fn=<SqueezeBackward0>)\n",
            "19 th loss is  tensor(25.3597, grad_fn=<SqueezeBackward0>)\n",
            "20 th loss is  tensor(23.9565, grad_fn=<SqueezeBackward0>)\n",
            "21 th loss is  tensor(22.6854, grad_fn=<SqueezeBackward0>)\n",
            "22 th loss is  tensor(21.5299, grad_fn=<SqueezeBackward0>)\n",
            "23 th loss is  tensor(20.4852, grad_fn=<SqueezeBackward0>)\n",
            "24 th loss is  tensor(19.5419, grad_fn=<SqueezeBackward0>)\n",
            "25 th loss is  tensor(18.6883, grad_fn=<SqueezeBackward0>)\n",
            "26 th loss is  tensor(17.9155, grad_fn=<SqueezeBackward0>)\n",
            "27 th loss is  tensor(17.2106, grad_fn=<SqueezeBackward0>)\n",
            "28 th loss is  tensor(16.5681, grad_fn=<SqueezeBackward0>)\n",
            "29 th loss is  tensor(15.9767, grad_fn=<SqueezeBackward0>)\n",
            "30 th loss is  tensor(15.4323, grad_fn=<SqueezeBackward0>)\n",
            "31 th loss is  tensor(14.9255, grad_fn=<SqueezeBackward0>)\n",
            "32 th loss is  tensor(14.4479, grad_fn=<SqueezeBackward0>)\n",
            "33 th loss is  tensor(13.9949, grad_fn=<SqueezeBackward0>)\n",
            "34 th loss is  tensor(13.5637, grad_fn=<SqueezeBackward0>)\n",
            "35 th loss is  tensor(13.1528, grad_fn=<SqueezeBackward0>)\n",
            "36 th loss is  tensor(12.7605, grad_fn=<SqueezeBackward0>)\n",
            "37 th loss is  tensor(12.3867, grad_fn=<SqueezeBackward0>)\n",
            "38 th loss is  tensor(12.0301, grad_fn=<SqueezeBackward0>)\n",
            "39 th loss is  tensor(11.6885, grad_fn=<SqueezeBackward0>)\n",
            "40 th loss is  tensor(11.3595, grad_fn=<SqueezeBackward0>)\n",
            "41 th loss is  tensor(11.0439, grad_fn=<SqueezeBackward0>)\n",
            "42 th loss is  tensor(10.7422, grad_fn=<SqueezeBackward0>)\n",
            "43 th loss is  tensor(10.4556, grad_fn=<SqueezeBackward0>)\n",
            "44 th loss is  tensor(10.1923, grad_fn=<SqueezeBackward0>)\n",
            "45 th loss is  tensor(9.9592, grad_fn=<SqueezeBackward0>)\n",
            "46 th loss is  tensor(9.7892, grad_fn=<SqueezeBackward0>)\n",
            "47 th loss is  tensor(9.7227, grad_fn=<SqueezeBackward0>)\n",
            "48 th loss is  tensor(9.9005, grad_fn=<SqueezeBackward0>)\n",
            "49 th loss is  tensor(10.2676, grad_fn=<SqueezeBackward0>)\n",
            "50 th loss is  tensor(11.6966, grad_fn=<SqueezeBackward0>)\n",
            "51 th loss is  tensor(13.1237, grad_fn=<SqueezeBackward0>)\n",
            "52 th loss is  tensor(19.3492, grad_fn=<SqueezeBackward0>)\n",
            "53 th loss is  tensor(21.1015, grad_fn=<SqueezeBackward0>)\n",
            "54 th loss is  tensor(38.3524, grad_fn=<SqueezeBackward0>)\n",
            "55 th loss is  tensor(15.4335, grad_fn=<SqueezeBackward0>)\n",
            "56 th loss is  tensor(23.2409, grad_fn=<SqueezeBackward0>)\n",
            "57 th loss is  tensor(20.8723, grad_fn=<SqueezeBackward0>)\n",
            "58 th loss is  tensor(34.2742, grad_fn=<SqueezeBackward0>)\n",
            "59 th loss is  tensor(12.5960, grad_fn=<SqueezeBackward0>)\n",
            "60 th loss is  tensor(15.9525, grad_fn=<SqueezeBackward0>)\n",
            "61 th loss is  tensor(13.8371, grad_fn=<SqueezeBackward0>)\n",
            "62 th loss is  tensor(18.5189, grad_fn=<SqueezeBackward0>)\n",
            "63 th loss is  tensor(14.9805, grad_fn=<SqueezeBackward0>)\n",
            "64 th loss is  tensor(20.3987, grad_fn=<SqueezeBackward0>)\n",
            "65 th loss is  tensor(15.1890, grad_fn=<SqueezeBackward0>)\n",
            "66 th loss is  tensor(20.2574, grad_fn=<SqueezeBackward0>)\n",
            "67 th loss is  tensor(14.2238, grad_fn=<SqueezeBackward0>)\n",
            "68 th loss is  tensor(18.0535, grad_fn=<SqueezeBackward0>)\n",
            "69 th loss is  tensor(12.6465, grad_fn=<SqueezeBackward0>)\n",
            "70 th loss is  tensor(15.1881, grad_fn=<SqueezeBackward0>)\n",
            "71 th loss is  tensor(11.0705, grad_fn=<SqueezeBackward0>)\n",
            "72 th loss is  tensor(12.8051, grad_fn=<SqueezeBackward0>)\n",
            "73 th loss is  tensor(9.8735, grad_fn=<SqueezeBackward0>)\n",
            "74 th loss is  tensor(11.1729, grad_fn=<SqueezeBackward0>)\n",
            "75 th loss is  tensor(9.0317, grad_fn=<SqueezeBackward0>)\n",
            "76 th loss is  tensor(10.0806, grad_fn=<SqueezeBackward0>)\n",
            "77 th loss is  tensor(8.6283, grad_fn=<SqueezeBackward0>)\n",
            "78 th loss is  tensor(9.5653, grad_fn=<SqueezeBackward0>)\n",
            "79 th loss is  tensor(8.3042, grad_fn=<SqueezeBackward0>)\n",
            "80 th loss is  tensor(9.1272, grad_fn=<SqueezeBackward0>)\n",
            "81 th loss is  tensor(8.0300, grad_fn=<SqueezeBackward0>)\n",
            "82 th loss is  tensor(8.8109, grad_fn=<SqueezeBackward0>)\n",
            "83 th loss is  tensor(7.7774, grad_fn=<SqueezeBackward0>)\n",
            "84 th loss is  tensor(8.5181, grad_fn=<SqueezeBackward0>)\n",
            "85 th loss is  tensor(7.6164, grad_fn=<SqueezeBackward0>)\n",
            "86 th loss is  tensor(8.4115, grad_fn=<SqueezeBackward0>)\n",
            "87 th loss is  tensor(7.4461, grad_fn=<SqueezeBackward0>)\n",
            "88 th loss is  tensor(8.2319, grad_fn=<SqueezeBackward0>)\n",
            "89 th loss is  tensor(7.3632, grad_fn=<SqueezeBackward0>)\n",
            "90 th loss is  tensor(8.2074, grad_fn=<SqueezeBackward0>)\n",
            "91 th loss is  tensor(7.3529, grad_fn=<SqueezeBackward0>)\n",
            "92 th loss is  tensor(8.2864, grad_fn=<SqueezeBackward0>)\n",
            "93 th loss is  tensor(7.4559, grad_fn=<SqueezeBackward0>)\n",
            "94 th loss is  tensor(8.5354, grad_fn=<SqueezeBackward0>)\n",
            "95 th loss is  tensor(7.6491, grad_fn=<SqueezeBackward0>)\n",
            "96 th loss is  tensor(8.8974, grad_fn=<SqueezeBackward0>)\n",
            "97 th loss is  tensor(7.9075, grad_fn=<SqueezeBackward0>)\n",
            "98 th loss is  tensor(9.2786, grad_fn=<SqueezeBackward0>)\n",
            "99 th loss is  tensor(8.1660, grad_fn=<SqueezeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGgz2PSSh5hE",
        "outputId": "460a4a4b-2fdc-43a2-e44b-795a125144e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "seq_net = torch.nn.Sequential(torch.nn.Linear(1,15), torch.nn.ReLU(), torch.nn.Linear(15,1))\n",
        "lossFunc = torch.nn.MSELoss()\n",
        "print(seq_net)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=15, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=15, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R1v1dhyib02",
        "outputId": "202397c8-6f07-4cd6-8f37-ebbb1ff991dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(epoch):\n",
        "  output = net(x)\n",
        "  loss = lossFunc(output, y)\n",
        "  net.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  for f in net.parameters():\n",
        "    f.data.sub_(learning_rate * f.grad.data)\n",
        "\n",
        "  print(i, 'th loss is ', loss.squeeze())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 th loss is  tensor(9.6817, grad_fn=<SqueezeBackward0>)\n",
            "1 th loss is  tensor(8.4218, grad_fn=<SqueezeBackward0>)\n",
            "2 th loss is  tensor(10.0459, grad_fn=<SqueezeBackward0>)\n",
            "3 th loss is  tensor(8.5776, grad_fn=<SqueezeBackward0>)\n",
            "4 th loss is  tensor(10.1625, grad_fn=<SqueezeBackward0>)\n",
            "5 th loss is  tensor(8.5229, grad_fn=<SqueezeBackward0>)\n",
            "6 th loss is  tensor(10.0174, grad_fn=<SqueezeBackward0>)\n",
            "7 th loss is  tensor(8.2886, grad_fn=<SqueezeBackward0>)\n",
            "8 th loss is  tensor(9.6420, grad_fn=<SqueezeBackward0>)\n",
            "9 th loss is  tensor(7.9251, grad_fn=<SqueezeBackward0>)\n",
            "10 th loss is  tensor(9.1206, grad_fn=<SqueezeBackward0>)\n",
            "11 th loss is  tensor(7.5038, grad_fn=<SqueezeBackward0>)\n",
            "12 th loss is  tensor(8.5570, grad_fn=<SqueezeBackward0>)\n",
            "13 th loss is  tensor(7.0801, grad_fn=<SqueezeBackward0>)\n",
            "14 th loss is  tensor(8.0201, grad_fn=<SqueezeBackward0>)\n",
            "15 th loss is  tensor(6.6980, grad_fn=<SqueezeBackward0>)\n",
            "16 th loss is  tensor(7.5594, grad_fn=<SqueezeBackward0>)\n",
            "17 th loss is  tensor(6.3880, grad_fn=<SqueezeBackward0>)\n",
            "18 th loss is  tensor(7.2043, grad_fn=<SqueezeBackward0>)\n",
            "19 th loss is  tensor(6.1533, grad_fn=<SqueezeBackward0>)\n",
            "20 th loss is  tensor(6.9576, grad_fn=<SqueezeBackward0>)\n",
            "21 th loss is  tensor(5.9978, grad_fn=<SqueezeBackward0>)\n",
            "22 th loss is  tensor(6.8141, grad_fn=<SqueezeBackward0>)\n",
            "23 th loss is  tensor(5.9183, grad_fn=<SqueezeBackward0>)\n",
            "24 th loss is  tensor(6.7655, grad_fn=<SqueezeBackward0>)\n",
            "25 th loss is  tensor(5.9067, grad_fn=<SqueezeBackward0>)\n",
            "26 th loss is  tensor(6.8033, grad_fn=<SqueezeBackward0>)\n",
            "27 th loss is  tensor(5.9505, grad_fn=<SqueezeBackward0>)\n",
            "28 th loss is  tensor(6.9072, grad_fn=<SqueezeBackward0>)\n",
            "29 th loss is  tensor(6.0403, grad_fn=<SqueezeBackward0>)\n",
            "30 th loss is  tensor(7.0629, grad_fn=<SqueezeBackward0>)\n",
            "31 th loss is  tensor(6.1563, grad_fn=<SqueezeBackward0>)\n",
            "32 th loss is  tensor(7.2396, grad_fn=<SqueezeBackward0>)\n",
            "33 th loss is  tensor(6.2790, grad_fn=<SqueezeBackward0>)\n",
            "34 th loss is  tensor(7.4104, grad_fn=<SqueezeBackward0>)\n",
            "35 th loss is  tensor(6.3823, grad_fn=<SqueezeBackward0>)\n",
            "36 th loss is  tensor(7.5401, grad_fn=<SqueezeBackward0>)\n",
            "37 th loss is  tensor(6.4382, grad_fn=<SqueezeBackward0>)\n",
            "38 th loss is  tensor(7.5903, grad_fn=<SqueezeBackward0>)\n",
            "39 th loss is  tensor(6.4220, grad_fn=<SqueezeBackward0>)\n",
            "40 th loss is  tensor(7.5342, grad_fn=<SqueezeBackward0>)\n",
            "41 th loss is  tensor(6.3233, grad_fn=<SqueezeBackward0>)\n",
            "42 th loss is  tensor(7.3659, grad_fn=<SqueezeBackward0>)\n",
            "43 th loss is  tensor(6.1422, grad_fn=<SqueezeBackward0>)\n",
            "44 th loss is  tensor(7.0968, grad_fn=<SqueezeBackward0>)\n",
            "45 th loss is  tensor(5.9099, grad_fn=<SqueezeBackward0>)\n",
            "46 th loss is  tensor(6.7646, grad_fn=<SqueezeBackward0>)\n",
            "47 th loss is  tensor(5.6401, grad_fn=<SqueezeBackward0>)\n",
            "48 th loss is  tensor(6.4040, grad_fn=<SqueezeBackward0>)\n",
            "49 th loss is  tensor(5.3591, grad_fn=<SqueezeBackward0>)\n",
            "50 th loss is  tensor(6.0426, grad_fn=<SqueezeBackward0>)\n",
            "51 th loss is  tensor(5.0969, grad_fn=<SqueezeBackward0>)\n",
            "52 th loss is  tensor(5.7165, grad_fn=<SqueezeBackward0>)\n",
            "53 th loss is  tensor(4.8633, grad_fn=<SqueezeBackward0>)\n",
            "54 th loss is  tensor(5.4339, grad_fn=<SqueezeBackward0>)\n",
            "55 th loss is  tensor(4.6624, grad_fn=<SqueezeBackward0>)\n",
            "56 th loss is  tensor(5.1997, grad_fn=<SqueezeBackward0>)\n",
            "57 th loss is  tensor(4.5079, grad_fn=<SqueezeBackward0>)\n",
            "58 th loss is  tensor(5.0256, grad_fn=<SqueezeBackward0>)\n",
            "59 th loss is  tensor(4.3950, grad_fn=<SqueezeBackward0>)\n",
            "60 th loss is  tensor(4.9011, grad_fn=<SqueezeBackward0>)\n",
            "61 th loss is  tensor(4.3209, grad_fn=<SqueezeBackward0>)\n",
            "62 th loss is  tensor(4.8311, grad_fn=<SqueezeBackward0>)\n",
            "63 th loss is  tensor(4.2887, grad_fn=<SqueezeBackward0>)\n",
            "64 th loss is  tensor(4.8147, grad_fn=<SqueezeBackward0>)\n",
            "65 th loss is  tensor(4.2979, grad_fn=<SqueezeBackward0>)\n",
            "66 th loss is  tensor(4.8513, grad_fn=<SqueezeBackward0>)\n",
            "67 th loss is  tensor(4.3477, grad_fn=<SqueezeBackward0>)\n",
            "68 th loss is  tensor(4.9391, grad_fn=<SqueezeBackward0>)\n",
            "69 th loss is  tensor(4.4354, grad_fn=<SqueezeBackward0>)\n",
            "70 th loss is  tensor(5.0679, grad_fn=<SqueezeBackward0>)\n",
            "71 th loss is  tensor(4.1568, grad_fn=<SqueezeBackward0>)\n",
            "72 th loss is  tensor(4.7059, grad_fn=<SqueezeBackward0>)\n",
            "73 th loss is  tensor(3.9144, grad_fn=<SqueezeBackward0>)\n",
            "74 th loss is  tensor(4.3913, grad_fn=<SqueezeBackward0>)\n",
            "75 th loss is  tensor(3.7190, grad_fn=<SqueezeBackward0>)\n",
            "76 th loss is  tensor(4.1396, grad_fn=<SqueezeBackward0>)\n",
            "77 th loss is  tensor(3.5552, grad_fn=<SqueezeBackward0>)\n",
            "78 th loss is  tensor(3.8731, grad_fn=<SqueezeBackward0>)\n",
            "79 th loss is  tensor(3.3767, grad_fn=<SqueezeBackward0>)\n",
            "80 th loss is  tensor(3.6145, grad_fn=<SqueezeBackward0>)\n",
            "81 th loss is  tensor(3.2032, grad_fn=<SqueezeBackward0>)\n",
            "82 th loss is  tensor(3.4396, grad_fn=<SqueezeBackward0>)\n",
            "83 th loss is  tensor(3.0882, grad_fn=<SqueezeBackward0>)\n",
            "84 th loss is  tensor(3.3361, grad_fn=<SqueezeBackward0>)\n",
            "85 th loss is  tensor(3.0268, grad_fn=<SqueezeBackward0>)\n",
            "86 th loss is  tensor(3.2980, grad_fn=<SqueezeBackward0>)\n",
            "87 th loss is  tensor(3.0161, grad_fn=<SqueezeBackward0>)\n",
            "88 th loss is  tensor(3.3167, grad_fn=<SqueezeBackward0>)\n",
            "89 th loss is  tensor(3.0508, grad_fn=<SqueezeBackward0>)\n",
            "90 th loss is  tensor(3.3897, grad_fn=<SqueezeBackward0>)\n",
            "91 th loss is  tensor(3.1298, grad_fn=<SqueezeBackward0>)\n",
            "92 th loss is  tensor(3.5164, grad_fn=<SqueezeBackward0>)\n",
            "93 th loss is  tensor(3.2522, grad_fn=<SqueezeBackward0>)\n",
            "94 th loss is  tensor(3.6951, grad_fn=<SqueezeBackward0>)\n",
            "95 th loss is  tensor(3.2852, grad_fn=<SqueezeBackward0>)\n",
            "96 th loss is  tensor(3.7518, grad_fn=<SqueezeBackward0>)\n",
            "97 th loss is  tensor(3.0925, grad_fn=<SqueezeBackward0>)\n",
            "98 th loss is  tensor(3.5253, grad_fn=<SqueezeBackward0>)\n",
            "99 th loss is  tensor(2.8168, grad_fn=<SqueezeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3oJeuzDjoBc",
        "outputId": "bc8fe161-6aaa-4081-c898-74d96b21a0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "seq_net = torch.nn.Sequential(torch.nn.Linear(1,15), torch.nn.ReLU(), torch.nn.Linear(15,1))\n",
        "lossFunc = torch.nn.MSELoss()\n",
        "print(seq_net)\n",
        "\n",
        "optim = torch.optim.SGD(seq_net.parameters(), lr=0.0611, momentum=0.9)\n",
        "for i in range(epoch):\n",
        "  output = seq_net(x)\n",
        "  loss = lossFunc(output, y)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  optim.step() #param updation\n",
        "  if ((i+1) % 4) == 0:\n",
        "    print(str(i+1) + ' th loss is '+ str(loss.squeeze()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=15, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=15, out_features=1, bias=True)\n",
            ")\n",
            "4 th loss is tensor(64.2278, grad_fn=<SqueezeBackward0>)\n",
            "8 th loss is tensor(38.8788, grad_fn=<SqueezeBackward0>)\n",
            "12 th loss is tensor(92.2776, grad_fn=<SqueezeBackward0>)\n",
            "16 th loss is tensor(101.5517, grad_fn=<SqueezeBackward0>)\n",
            "20 th loss is tensor(80.9151, grad_fn=<SqueezeBackward0>)\n",
            "24 th loss is tensor(79.7701, grad_fn=<SqueezeBackward0>)\n",
            "28 th loss is tensor(80.3601, grad_fn=<SqueezeBackward0>)\n",
            "32 th loss is tensor(79.3924, grad_fn=<SqueezeBackward0>)\n",
            "36 th loss is tensor(79.6857, grad_fn=<SqueezeBackward0>)\n",
            "40 th loss is tensor(79.1633, grad_fn=<SqueezeBackward0>)\n",
            "44 th loss is tensor(79.1659, grad_fn=<SqueezeBackward0>)\n",
            "48 th loss is tensor(78.9998, grad_fn=<SqueezeBackward0>)\n",
            "52 th loss is tensor(78.9705, grad_fn=<SqueezeBackward0>)\n",
            "56 th loss is tensor(78.9683, grad_fn=<SqueezeBackward0>)\n",
            "60 th loss is tensor(78.9324, grad_fn=<SqueezeBackward0>)\n",
            "64 th loss is tensor(78.9266, grad_fn=<SqueezeBackward0>)\n",
            "68 th loss is tensor(78.9025, grad_fn=<SqueezeBackward0>)\n",
            "72 th loss is tensor(78.9131, grad_fn=<SqueezeBackward0>)\n",
            "76 th loss is tensor(78.9028, grad_fn=<SqueezeBackward0>)\n",
            "80 th loss is tensor(78.9018, grad_fn=<SqueezeBackward0>)\n",
            "84 th loss is tensor(78.8993, grad_fn=<SqueezeBackward0>)\n",
            "88 th loss is tensor(78.8990, grad_fn=<SqueezeBackward0>)\n",
            "92 th loss is tensor(78.8982, grad_fn=<SqueezeBackward0>)\n",
            "96 th loss is tensor(78.8968, grad_fn=<SqueezeBackward0>)\n",
            "100 th loss is tensor(78.8972, grad_fn=<SqueezeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efikoyoikXF8",
        "outputId": "916c37fd-493b-4446-9dcc-19d7c814bab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as functional\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x = torch.randn(200,1) #Generate random data\n",
        "y = x.pow(4) + 0.05*torch.rand(x.size()) #add some noise to the output\n",
        "epoch = 1500\n",
        "learning_rate = 0.05\n",
        "\n",
        "#Manual Method. Involves building the layer architecture:\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self,n_input,n_hidden,n_output):\n",
        "        '''\n",
        "        Intialize 2 layer neural net\n",
        "        '''\n",
        "        super(Net,self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_input,n_hidden)\n",
        "        self.output = torch.nn.Linear(n_hidden,n_output)\n",
        "    \n",
        "    def forward(self,input):\n",
        "        '''\n",
        "        input=> The input to compute forward prop on\n",
        "        returns => output after forward prop\n",
        "        '''\n",
        "        hidden = functional.relu(self.hidden(input))\n",
        "        output = self.output(hidden)\n",
        "        return output\n",
        "\n",
        "net = Net(1,15,1)\n",
        "print(net)\n",
        "\n",
        "#Antoher method using Sequential function. Much less work\n",
        "seq_net = torch.nn.Sequential(torch.nn.Linear(1,15),\n",
        "                              torch.nn.ReLU(),\n",
        "                              torch.nn.Linear(15,1)\n",
        "                             )\n",
        "\n",
        "print(seq_net) #This should be same as net\n",
        "\n",
        "lossFunc = torch.nn.MSELoss()  #Mean Squared Error Function\n",
        "optim = torch.optim.SGD(seq_net.parameters(),lr=0.0611,momentum=0.9) #SGD optimizer with momentum\n",
        "\n",
        "for i in range(epoch):\n",
        "    '''\n",
        "    To cange the network used(net or seq_net), you just need to replace one network with another\n",
        "    '''\n",
        "    output = seq_net(x) #forward Prop\n",
        "    loss = lossFunc(output,y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    #Manual Updation of parameters\n",
        "    '''\n",
        "    for f in seq_net.parameters():\n",
        "        f.data.sub_(f.grad.data*learning_rate)\n",
        "    '''\n",
        "    optim.step()\n",
        "    if (i+1)%4==0 :\n",
        "        print(str((i+1))+'th loss',loss.squeeze())\n",
        "    if i % 5 == 0:\n",
        "        # plot and show learning process\n",
        "        plt.cla()\n",
        "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
        "        plt.plot(x.data.numpy(), output.data.numpy(), 'r-', lw=5)\n",
        "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color':  'red'})\n",
        "        plt.pause(0.1)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (hidden): Linear(in_features=1, out_features=15, bias=True)\n",
            "  (output): Linear(in_features=15, out_features=1, bias=True)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=15, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=15, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cf6741f9e8ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss=%.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'color'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;34m'red'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu7sm-CnlYQQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}